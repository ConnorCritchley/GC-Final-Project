{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9752364b-78dd-434d-bb43-74513e001afd",
   "metadata": {},
   "source": [
    "# Grand Circus Final Project\n",
    "### Car Crash and Safety Data Comparisons/Evaluations\n",
    "\n",
    "This project aims to compare safety ratings from crash tests to actual data of fatal crashes. The use of fatal crash data is better suited for hard crashes where occupant life is and was in danger, providing more relevant data entries compared to fender benders or other minimal 'traffic incidents'. This analysis could be useful for car buyers, car manufactureres, government testers, and insurance companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1e353-3240-4956-9f06-80bf359f3a82",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "To start the ETA process, data but be extracted and placed into usable structures. To do this, we will be importing the data from the api(s) and any other flat file sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf99cb-0957-45e9-8c5b-25535059fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Begin pulling make names and ID's for internal use\n",
    "# Definitions endpoint query\n",
    "make_url = \"https://crashviewer.nhtsa.dot.gov/CrashAPI/definitions/GetVariableAttributes?variable=make&caseYear=2021&format=json\"\n",
    "\n",
    "# Get response\n",
    "response = requests.get(make_url)\n",
    "# Turn response into json\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc83b33-c8d6-4b97-a54e-ca3a1623f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drill down json to list of dictionary\n",
    "results = data['Results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327ea98-3fd8-4326-8512-4e82f3d1fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into lists\n",
    "id_list = []\n",
    "name_list = []\n",
    "for entry in results:\n",
    "    id_list.append(int(entry['ID']))\n",
    "    name_list.append(entry['TEXT'])\n",
    "\n",
    "# Make columns dictionary based on lists\n",
    "data = {'MakeID': id_list, 'Name': name_list}\n",
    "\n",
    "# Create df using dictionary\n",
    "manufacturer_df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by Id instead of name\n",
    "manufacturer_df = manufacturer_df.sort_values(by=['MakeID'])\n",
    "manufacturer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87967865-162f-46ae-bc63-2f654e945c37",
   "metadata": {},
   "source": [
    "## Only Taking Top 11 Best-Selling Makes\n",
    "Since the API contains data for all involved in crashes, such as the American Motors Ambassador made from 1952-1974, a fair portion of vehicles are not statistically relevant, or would be outwighed by more common vehicles. To prevent a weighting issing where more prevalent vehicles scew results to thinking more crashes are common, we will be using some of the most popular makes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1e6b5-74e3-4e5a-ac66-54fab27ecd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = ['Nissan/Datsun', 'Toyota', 'KIA', 'Honda', 'Subaru', 'Ford', 'Chevrolet', 'Hyundai', 'Jeep / Kaiser-Jeep / Willys- Jeep', 'GMC', 'Dodge']\n",
    "new_df = manufacturer_df[manufacturer_df['Name'].isin(to_keep)]\n",
    "manufacturer_df = new_df\n",
    "manufacturer_df.rename(columns={'Name': 'MakeName'}, inplace=True)\n",
    "manufacturer_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171ec78-5dc6-492c-a643-8411b831d4b8",
   "metadata": {},
   "source": [
    "### Fetching Model IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77afb5a-cb67-4eef-ada7-921085d3c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "all_models = []\n",
    "for make_ID in manufacturer_df['MakeID']:\n",
    "    model_url = f'https://crashviewer.nhtsa.dot.gov/CrashAPI/definitions/GetVariableAttributesForModel?variable=model&caseYear=2021&make={make_ID}&format=json'\n",
    "    response = requests.get(model_url)\n",
    "    model_data = response.json()\n",
    "    \n",
    "    results_model = model_data.get('Results') \n",
    "\n",
    "    time.sleep(1)\n",
    "    for model in results_model:\n",
    "        all_models.append({\n",
    "            'MakeID': make_ID,\n",
    "            'Models': model[0:]\n",
    "        })\n",
    "# Drill down into JSON\n",
    "drill_down = all_models[0]['Models']\n",
    "drill_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f975846-4c4f-402c-9e31-2bb99fedbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(all_models).sort_values(by='MakeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296eff43-371d-498c-8135-b035e2aadc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge manufacturer_df & models_df\n",
    "merged_df = pd.merge(manufacturer_df, models_df, on=\"MakeID\", how=\"left\")\n",
    "merged_df = merged_df.sort_values(by='MakeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6bdb8-c6af-4eb7-aae7-690453516390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the Models column to separate rows\n",
    "exploded_df = merged_df.explode('Models')\n",
    "exploded_df.reset_index(inplace=True)\n",
    "exploded_df.drop('index', axis=1, inplace=True) \n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe24db-09e0-476f-b335-46dc31bf37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ID and MODELNAME from the dictionaries in the Models column\n",
    "exploded_df['ModelID'] = exploded_df['Models'].apply(lambda x: x['ID'] if isinstance(x, dict) else None)\n",
    "exploded_df['ModelName'] = exploded_df['Models'].apply(lambda x: x['MODELNAME'] if isinstance(x, dict) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8e488-2222-455c-b971-e511e13a31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original Models column\n",
    "df = exploded_df.drop(columns=['Models'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0f76b-8e8c-4a34-a9a0-1ce2d2d27692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c39fc4-48ac-49f0-8e01-838174e86671",
   "metadata": {},
   "source": [
    "## Bodytype fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45048b-b386-4473-b7b5-0287a847c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every car needs a body type to query the api with\n",
    "import os.path\n",
    "import tqdm\n",
    "base_url = \"https://crashviewer.nhtsa.dot.gov/CrashAPI/definitions/GetVariableAttributesForbodyType\"\n",
    "\n",
    "bodytypes = []\n",
    "\n",
    "# loop through every row in dataframe\n",
    "for car in tqdm.tqdm(range(len(df))):\n",
    "    if os.path.isfile(\"body-types.json\"):\n",
    "        break\n",
    "    # for every car in dataframe     df.iloc[0]['A']\n",
    "    params = f\"?variable=bodytype&make={df.iloc[car]['MakeID']}&model={df.iloc[car]['ModelID']}&format=json\"\n",
    "    # get \"BODY_ID\" from responses and append to each row\n",
    "    # Get response\n",
    "    response = requests.get(base_url + params)\n",
    "\n",
    "    # check if successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "    # Turn response into json\n",
    "    data = response.json()\n",
    "\n",
    "    # drill down\n",
    "    results = data['Results'][0]\n",
    "\n",
    "    # pull data from each bodytype per car\n",
    "    # format is going to be a list of dictionaries, such that the bodytypes list will be like bodytypes[car][dictionary response]\n",
    "    extracted = {entry['BODY_DEF'].split('(')[0].strip(): entry['BODY_ID'] for entry in data['Results'][0]}\n",
    "\n",
    "    # append extracted to main list\n",
    "    bodytypes.append(extracted)\n",
    "\n",
    "    # sleep for polite scraping\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e29bc1-0828-44c1-bf99-48bf60217e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"body-types.json\"):\n",
    "    with open(\"body-types.json\", \"w\") as outfile:\n",
    "        outfile.write(json.dumps(bodytypes))\n",
    "else:\n",
    "    with open('body-types.json', 'r') as openfile:\n",
    "        bodytypes = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f1562-1f68-4f1b-adb9-2a24b83e7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "BodyDef = []\n",
    "BodyId = []\n",
    "for dictionary in bodytypes:\n",
    "    for key, value in dictionary.items():\n",
    "        BodyDef.append(key)\n",
    "        BodyId.append(int(value))\n",
    "        break\n",
    "        \n",
    "df['BodyID'] = BodyId\n",
    "df['BodyType'] = BodyDef\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5f2a2-b40f-4063-b6db-867cd518f633",
   "metadata": {},
   "source": [
    "## Getting Crashes Per Year Per Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dc966-5b02-4c3a-a2d1-bdb6a1f53ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add crash totals per model to above dataframe \n",
    "# this will be done by simply tallying responses for each car\n",
    "# Since the api has a max return limit, querying by each year (2010-onwards) will ensure all data is gathered, and allow for year grouping\n",
    "\n",
    "# Base URL for NHTSA API\n",
    "base_url = \"https://crashviewer.nhtsa.dot.gov/CrashAPI/FARSData/GetFARSData\"\n",
    "\n",
    "# Function to get fatal crash data for a specific year and state\n",
    "def get_fatal_crashes(year, state):\n",
    "    params = f\"?dataset=Vehicle&FromYear={year}&ToYear={year}&state={state}&format=json\"\n",
    "    response = requests.get(base_url + params)\n",
    "\n",
    "    # Check for issues\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "\n",
    "    data = response.json()\n",
    "    if response[\"Message\"] == \"Results returned successfully\":\n",
    "        return data.get(\"Results\", [])\n",
    "    else:\n",
    "        print(f\"Error at api call for {year} and {state}!\")\n",
    "\n",
    "\n",
    "years = range(2010, 2010)\n",
    "states = range(1, 57)\n",
    "\n",
    "# Adding year columns to DataFrame for fatal crashes\n",
    "for year in years:\n",
    "    df[str(year)] = 0\n",
    "\n",
    "for year in tqdm.tqdm(years):\n",
    "    for state in states:\n",
    "        crash_data = get_fatal_crashes(year, state)\n",
    "        os.sleep(10)\n",
    "        \n",
    "        # Iterate over each vehicle in the crash data\n",
    "        for vehicle_list in crash_data:\n",
    "            for vehicle in vehicle_list:  # vehicle_list contains crash details for a particular vehicle\n",
    "                make = vehicle['MAKENAME']  # We are using 'MAKENAME' from the response\n",
    "                model = vehicle['MODELNAME']  # We are using 'MODELNAME' from the response\n",
    "                deaths = int(vehicle['DEATHS'])  # Convert deaths to an integer\n",
    "                \n",
    "                # Find the row in the dataframe that matches the make and model\n",
    "                vehicle_row = df[(df['MakeName'] == make) & (df['ModelName'] == model)]\n",
    "\n",
    "                # If the vehicle is found, update the deaths for that year\n",
    "                if not vehicle_row.empty:\n",
    "                    df.loc[vehicle_row.index, str(year)] += deaths\n",
    "\n",
    "        # Periodically save the dataframe after processing each state\n",
    "        df.to_csv(\"fatal_crashes.csv\", mode='w', header=True, index=False)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be850427-118e-4c15-91fe-1fe14626fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(year_totals[2011])\n",
    "# {2011: [23, 34, 12, 55, 23, 4534]}\n",
    "year_df = pd.DataFrame.from_dict(year_totals)\n",
    "year_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d52573-2b2d-4f0b-83dc-a37d8ea495a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df.info()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b479409-01f9-4f86-a02e-19580bbe000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25172caa-31bf-4cdc-9a89-21a404ea25a0",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "Now that we have usable, workable data, we can begin cleaning and organizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56862a-5069-42b1-87fd-ae4dd22dea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation code\n",
    "\n",
    "# Drop any unneeded columns/rows\n",
    "    # duplicates\n",
    "    # nulls\n",
    "    # outliers\n",
    "\n",
    "# Merge/Join Data into one dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ba1ff-478a-4523-9053-dc63d992e3f9",
   "metadata": {},
   "source": [
    "## Load\n",
    "With curated data, can now be loaded into postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f00958-3245-43c7-b1cc-552c33bd200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql alchemy and stuff\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "with open('credentials.json', 'r') as openfile:\n",
    "    credentials = json.load(openfile)\n",
    "\n",
    "\n",
    "TABLE_NAME = 'car_data'\n",
    "\n",
    "DB_NAME = \"safecars\"\n",
    "DB_USER = credentials['user']\n",
    "DB_PASS = credentials['pass']\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# create engine with defined macros\n",
    "engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME}\")\n",
    "# send the df over\n",
    "#df.to_sql(name=TABLE_NAME,\n",
    "          con=engine,\n",
    "          index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e2a1b-7c46-4372-9b03-7cb4e2508b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM car_data\" # simple query for all rows\n",
    "#sql_df = pd.read_sql(sql, engine) # make a df from postgres\n",
    "#sql_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
